---
layout: docwithnav-mqtt-broker
title: TBMQ 架构
description: TBMQ 架构

---

* TOC
{:toc}

### 简介

本文解释了 TBMQ 的架构结构，分解了数据如何在不同组件之间移动，并概述了核心架构选择。
TBMQ 经过精心设计，以实现以下属性：

* **可扩展性**：它是一个使用尖端开源技术构建的水平可扩展平台；
* **容错性**：没有单点故障，集群中的每个代理（节点）在功能方面都是相同的；
* **稳健性和效率**：单个服务器节点可以管理数百万个客户端并每秒处理数十万条消息。
  值得注意的是，TBMQ 集群可以处理至少 [100M 客户端和每秒 3M 条传入消息](/docs/mqtt-broker/reference/100m-connections-performance-test/) 的巨大流量；
* **持久性**：确保数据保持准确和一致非常重要。
  TBMQ 依赖 Kafka 队列实现来提供极高的消息持久性，确保数据永不丢失。

#### 架构图

下图显示了代理的关键部分和消息传输的路由。

![image](/images/mqtt-broker/architecture/tbmq-architecture.png)

### 动机

在 GridLinks 公司，我们对各种物联网需求和案例的广泛专业知识和清晰理解
帮助我们确定了我们的客户开发其解决方案的两个主要场景。
在第一个场景中，大量设备生成大量消息，这些消息被特定应用程序使用，从而形成扇入模式。
另一方面，第二个场景涉及大量设备订阅特定更新或通知。
这会导致一些传入请求导致大量传出数据。这种情况称为扇出模式。
了解这些场景后，我们有意设计 TBMQ 以使其非常适合这两种场景。

我们的设计原则侧重于确保代理的容错性和高可用性。
我们希望实现一种可靠的消息处理机制，能够处理数据流中参与者可能出现的任何潜在故障。
此外，我们优先支持分布式和分区处理，以便随着业务增长轻松实现水平可扩展性。

我们希望我们的代理支持高吞吐量处理，确保向客户端低延迟地传递消息，
并提供承受发布客户端峰值负载的能力，同时确保为离线客户端提供备份存储。

在我们的设计中，确保数据持久性和复制至关重要。我们的目标是建立一个系统，一旦代理确认收到消息，它就会保持安全且不会丢失。
即使 QoS 0 消息已传递给代理，但尚未到达订阅者且代理重新启动，该消息已保存，一旦代理再次运行，该消息将传递给订阅者。

我们不想重新发明轮子来满足我们对持久性和处理层的全部要求。
相反，我们希望通过选择世界领先的平台来实现所有这些目标。
考虑到这些因素，我们对底层系统做出了战略决策，该系统可以与 MQTT 层无缝集成。

### TBMQ 简而言之是如何工作的

为了确保满足上述要求并防止在客户端或代理故障的情况下丢失消息，
TBMQ 使用 [Kafka](https://kafka.apache.org/) 的强大功能作为其底层基础设施。

Kafka 在 MQTT 工作流的各个阶段发挥着至关重要的作用。值得注意的是，客户端会话和订阅存储在专用的 [Kafka 主题](#kafka-主题) 中。
通过利用这些 Kafka 主题，所有代理节点都可以轻松访问客户端会话和订阅的最新状态，
允许它们维护本地副本以进行高效的消息处理和传递。
如果客户端与特定代理节点失去连接，其他节点会根据最新状态无缝地继续操作。
此外，集群中新添加的代理节点在激活后会获取此重要信息。

客户端订阅在 MQTT 发布/订阅模式中具有重要意义。
为了优化性能，TBMQ 采用 [Trie](#subscriptions-trie) 数据结构，
能够有效地将客户端订阅持久化到内存中，并便于快速访问相关主题模式。

当发布者客户端发送 _PUBLISH_ 消息时，它存储在名为 **tbmq.msg.all** 的初始 Kafka 主题中。
一旦 Kafka 确认消息的持久性，代理就会立即向发布者回复 _PUBACK_/_PUBREC_ 消息或根本不回复，
具体取决于所选的服务质量 (QoS) 级别。

随后，充当 Kafka 使用者的单独线程从上述 Kafka 主题检索消息并利用
Subscription Trie 数据结构来识别预期的接收者。
根据下面描述的 [客户端类型](/docs/mqtt-broker/user-guide/mqtt-client-type/)（**DEVICE** 或 **APPLICATION**）和持久性选项，
代理要么将消息重定向到另一个特定 Kafka 主题，要么直接将其传递给接收者。

#### 非持久性客户端

当 _CONNECT_ 数据包中满足以下条件时，客户端被归类为非持久性客户端：

对于 **MQTT v3.x**：
* `clean_session` 标志设置为 _true_。

对于 **MQTT v5**：
* `clean_start` 标志设置为 _true_，并且 `sessionExpiryInterval` 设置为 _0_ 或未指定。

对于非持久性客户端，所有针对它们的邮件都会直接发布，而无需进行额外的持久性处理。
值得注意的是，非持久性客户端只能是 **DEVICE** 类型。

![image](/images/mqtt-broker/architecture/tbmq-non-persistent-dev.png)

#### 持久性客户端

不满足上述非持久性条件的 MQTT 客户端被归类为持久性客户端。
让我们深入了解持久性客户端的条件：

对于 **MQTT v3.x**：
* `clean_session` 标志设置为 _false_。

对于 **MQTT v5 客户端**：
* `sessionExpiryInterval` 大于 _0_（无论 `clean_start` 标志如何）。
* `clean_start` 标志设置为 _false_，并且 `sessionExpiryInterval` 设置为 _0_ 或未指定。

#### 客户端类型

基于我们在物联网生态系统中的渊博知识和专业知识以及成功实施大量物联网案例，我们将 MQTT 客户端分为两大类：

* **DEVICE** 客户端主要从事发布大量消息，同时订阅数量有限的主题，消息速率相对较低。
  这些客户端通常与物联网设备或传感器相关联，这些设备或传感器经常向 TBMQ 传输数据。

* **APPLICATION** 客户端专门订阅消息速率高的主题。
  它们通常要求在客户端离线时将消息持久化，以便稍后传递，确保关键数据的可用性。
  APPLICATION 客户端通常用于实时分析、数据处理或其他应用程序级功能。

因此，我们做出了战略决策，通过分离这两种类型客户端的处理流程来优化性能。

##### DEVICE 客户端

![image](/images/mqtt-broker/architecture/tbmq-persistent-dev.png)

对于 **DEVICE** 持久性客户端，我们使用 **tbmq.msg.persisted** Kafka 主题作为处理从 **tbmq.msg.all** 主题中提取的已发布消息的一种方式。
充当 Kafka 使用者的专用线程检索这些消息并将它们存储在用于持久性存储的 [PostgreSQL](#postgresql-数据库) 数据库中。
这种方法特别适用于 DEVICE 客户端，因为它们通常不需要大量接收消息，并且可能不关心离线期间的消息丢失。
使用这种方法可以帮助我们在 DEVICE 客户端重新连接时顺利恢复存储的消息。
同时，它确保在涉及低传入消息速率的场景中具有良好的性能。

##### APPLICATION 客户端

![image](/images/mqtt-broker/architecture/tbmq-app.png)

对于 **APPLICATION** 持久性客户端，我们选择了一种不同的方法。
为每个客户端创建一个专用的 Kafka 主题，从 **tbmq.msg.all** 主题中提取的每个消息都针对特定 APPLICATION 存储在相应的 Kafka 主题中。
随后，为每个 APPLICATION 分配一个单独的线程（Kafka 使用者）。这些线程从相应的 Kafka 主题检索消息并将它们传递给各自的客户端。
这种方法通过确保有效的消息传递来显着提高性能。

通过使用这种方法，我们可以保证不会丢失任何消息，即使客户端遇到暂时的问题也是如此。
由于其持久性，存储在 Kafka 主题中的消息本质上始终可用。

值得注意的是，APPLICATION 客户端只能归类为 [持久性](#持久性-客户端)。

对于这两种类型的客户端，我们提供了可配置的工具来控制每个客户端的消息持久性和存储持续时间。
您可以参考以下环境变量来调整这些设置：
* TB_KAFKA_APP_PERSISTED_MSG_TOPIC_PROPERTIES；
* MQTT_PERSISTENT_SESSION_DEVICE_PERSISTED_MESSAGES_LIMIT；
* MQTT_PERSISTENT_SESSION_DEVICE_PERSISTED_MESSAGES_TTL。

有关更详细信息，请参阅以下 [文档](/docs/mqtt-broker/install/config/) 中提供的配置。

#### Kafka 主题

以下是 TBMQ 中使用的 Kafka 主题的综合列表，以及它们各自的描述。

* **tbmq.msg.all** - 用于存储从 MQTT 客户端发布到代理的所有消息的主题。
* **tbmq.msg.app. + ${client_id}** - 用于存储 APPLICATION 客户端应根据其订阅接收的消息的主题。
* **tbmq.msg.app.shared. + ${topic_filter}** - 用于存储 APPLICATION 客户端应根据其常见的共享订阅接收的消息的主题。
* **tbmq.msg.persisted** - 用于存储 DEVICE 持久性客户端应根据其订阅接收的消息的主题。
* **tbmq.msg.retained** - 用于存储所有保留消息的主题。与 MQTT 保留消息功能相关。
* **tbmq.client.session** - 用于存储所有客户端会话的主题。
* **tbmq.client.subscriptions** - 用于存储所有客户端订阅的主题。
* **tbmq.client.session.event.request** - 用于存储事件（如 _CONNECTION_REQUEST_、_DISCONNECTION_REQUEST_、_CLEAR_SESSION_REQUEST_ 等）的主题，用于所有客户端的会话。
* **tbmq.client.session.event.response. + ${service_id}** - 用于存储发送到目标客户端当前连接到的特定代理节点的先前主题的事件的响应的主题。
* **tbmq.client.disconnect. + ${service_id}** - 用于存储强制客户端断开连接事件的主题（由来自 UI/API 的管理员请求或会话冲突引起）。
* **tbmq.msg.downlink.basic. + ${service_id}** - 用于从一个代理节点发送消息到另一个代理节点的主题，DEVICE 订阅者当前连接到该代理节点。
* **tbmq.msg.downlink.persisted. + ${service_id}** - 用于从一个代理节点发送消息到另一个代理节点的主题，DEVICE 持久性订阅者当前连接到该代理节点。
* **tbmq.sys.app.removed** - 用于处理删除 APPLICATION 客户端主题的事件的主题。当客户端将其类型从 APPLICATION 更改为 DEVICE 时使用。
* **tbmq.sys.historical.data** - 用于历史数据统计的主题（例如，从集群中的每个代理节点发布的传入消息数、传出消息数等），以计算每个集群的总值。

#### PostgreSQL 数据库

TBMQ 使用 [PostgreSQL](https://www.postgresql.org/) 数据库来存储不同的实体，例如用户、用户凭据、MQTT 客户端凭据以及 DEVICE 的已发布消息等。

值得注意的是，Postgres 作为 SQL 数据库，在消息持久性速度方面存在一定的限制，
特别是它可以处理的每秒写入数。应该注意的是，Postgres 在这方面无法与 Kafka 的性能相匹配。
根据我们的经验，我们观察到每秒大约 5-6k 次操作的限制，具体取决于 Postgres 安装的硬件配置。

考虑到这些限制，我们建议将 APPLICATION 客户端用于超过上述性能阈值的使用案例。
APPLICATION 客户端及其专用的 Kafka 主题和用于消息传递的单独线程，
为需要强大消息持久性的场景提供更高的性能和可扩展性。

在未来的版本中，我们计划为客户端消息添加更多第三方持久性存储选项。
我们的目标是包括更好、更可靠的解决方案，以满足更广泛的需求。

#### Web UI

TBMQ 提供了一个用户友好且轻量级的图形用户界面 (GUI)，
以一种直观且高效的方式简化了代理的管理。
此 GUI 提供了几个关键功能来促进代理管理：

* MQTT 客户端凭据管理：用户可以通过 GUI 轻松管理 MQTT 客户端凭据，允许根据需要创建、修改和删除客户端凭据。
* 客户端会话和订阅控制：GUI 使管理员能够监视和控制客户端会话的状态，包括终止和管理活动客户端连接。
  它还提供管理客户端订阅的功能，允许添加、删除和修改订阅。
* 共享订阅管理：GUI 包括用于管理共享订阅的工具。
  管理员可以创建和管理应用程序共享订阅实体，从而促进将消息有效地分发给多个订阅的 APPLICATION 类型的客户端。
* 保留消息管理：GUI 允许管理员管理保留消息，这些消息由代理保存并传递给新订阅者。

除了这些管理功能外，GUI 还提供监视仪表板，提供有关代理性能的全面统计数据和见解。
这些仪表板提供关键指标和可视化，以便实时监视基本代理统计数据，
使管理员能够更好地了解系统的运行状况和性能。

这些功能的结合使 GUI 成为以用户友好且高效的方式管理、配置和监视 TBMQ 的宝贵工具。

#### Subscriptions Trie

在 TBMQ 中，所有客户端订阅都从 Kafka 主题中使用，并存储在内存中的 Trie 数据结构中。
Trie 按层次结构组织主题过滤器，每个节点表示主题过滤器中的一个级别。

当从 Kafka 读取 _PUBLISH_ 消息时，代理需要识别具有与已发布消息的主题名称相关的订阅的所有客户端，以确保他们收到消息。
Trie 数据结构能够根据主题名称有效地检索客户端订阅。
一旦识别出相关的订阅，消息的副本就会转发给每个相应的客户端。

这种方法确保了高性能的消息处理，因为它允许快速准确地确定需要接收特定消息的客户端。
然而，值得注意的是，这种方法需要代理增加内存消耗，因为要存储 Trie 数据结构。

有关 Trie 数据结构的更多详细信息，您可以参考提供的 [链接](https://en.wikipedia.org/wiki/Trie)。

#### Actor 系统

TBMQ 利用 Actor 系统作为实现负责处理 MQTT 客户端的 actor 的底层机制。
采用 Actor 模型能够有效且并发地处理从客户端接收的消息，从而确保高性能操作。

代理使用其自己的 Actor 系统的自定义实现，专门设计来满足 TBMQ 的要求。
在这个系统中，存在两种不同类型的 actor：

* **Client Actors**：对于每个连接的 MQTT 客户端，都会创建一个相应的 Client actor。
  这些 actor 负责处理主要消息类型，例如 _CONNECT_、_SUBSCRIBE_、_UNSUBSCRIBE_、_PUBLISH_ 等。
  Client actor 处理与 MQTT 客户端的交互，并帮助执行关联的消息操作。
* **Persisted Device Actors**：除了 Client actor 之外，还会为所有归类为持久性的 DEVICE 客户端创建额外的 Persisted Device actor。
  这些 actor 专门指定来管理与持久性相关的操作，并处理持久性 DEVICE 客户端的消息存储和检索。

通过 Actor 系统和各种 actor 类型，TBMQ 可以有效地并发处理消息，确保在处理客户端交互时具有最佳性能和快速响应。

有关 Actor 模型的更多见解，您可以参考提供的 [链接](https://en.wikipedia.org/wiki/Actor_model)。

### 独立模式与集群模式

TBMQ 被设计为可水平扩展，允许自动将新的代理节点添加到集群中。
集群中的所有节点都是相同的，并共享整体负载，确保客户端连接和消息处理的负载均衡分布。

代理的设计消除了对“主”或“协调器”进程的需求，因为没有负责管理其他进程的层次结构或中心节点。
这种分散的方法消除了单点故障的存在，增强了系统的整体稳健性和容错性。

为了处理客户端连接请求，可以使用您选择的负载均衡器。
负载均衡器将传入的客户端连接分布到所有可用的 TBMQ 节点，
均匀地分布工作负载并最大限度地提高资源利用率。

如果客户端失去与特定代理节点的连接（例如，由于节点关闭、删除或故障），
它可以轻松地重新连接到集群中的任何其他健康节点。
这种无缝重新连接功能确保了连接的客户端的连续操作和不间断服务，
因为它们可以与集群中的任何可用节点建立连接。

通过利用水平可扩展性、负载平衡和新节点的自动发现，
TBMQ 为处理大规模部署中的 MQTT 通信提供了高度可扩展且弹性的架构。

### 编程语言

TBMQ 的后端使用 Java 17 实现。TBMQ 的前端使用 Angular 15 框架开发为 SPA。